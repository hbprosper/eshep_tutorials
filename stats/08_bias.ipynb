{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Bias and Consistency\n",
    " >__Created__:  25 June 2018, ESHEP 18, Maratea, Italy,  Harrison B. Prosper\n",
    " \n",
    " \n",
    "## Introduction\n",
    "An important property of maximimum likelihood estimates is invariance with respect to a change of variables. Suppose  that\n",
    "$y = g(s)$. Given the maximum likelihood estimate (MLE), $\\hat{s}$, of $s$, the\n",
    "MLE of  $y$ is simply $\\hat{y} = g(\\hat{s})$. This property is very convenient because\n",
    "it makes it possible to maximize the likelihood using any convenient parameterization, .e.g. $s$, and obtain the  MLE\n",
    "of the parameter of interest, e.g. $y$, by simply plugging $\\hat{s}$ into $g(s)$. \n",
    "For example, $\\hat{s} = N$ is the MLE of the Poisson mean $s$, where \n",
    "\\begin{align}\n",
    "\t\\textrm{Poisson}(N, s) &= \\frac{e^{-s} s^N}{N!}.\n",
    "\\end{align}\n",
    "It is readily verified that $\\hat{s} = N$ is an unbiased estimate of the Poisson mean $s$. The MLE of its standard deviation, however, is biased. \n",
    "\n",
    "The standard deviation of a Poisson distribution is $y = \\sqrt{s}$. Therefore, the MLE is $\\hat{y} = \\sqrt{\\hat{s}} = \\sqrt{N}$.  As shown below, this estimate is indeed biased,  but, it is \n",
    "consistent, that is, the bias goes to zero as $N \\rightarrow \\infty$.\n",
    "\n",
    "## Bias and Consistency\n",
    "\n",
    "By definition, the __bias__ is given by\n",
    " \\begin{align}\n",
    "\t\\textrm{bias} \t&\\equiv \\langle  \\hat{s}  - s \\rangle, \\nonumber\\\\\n",
    "\t\t\t\t&= \\langle  \\hat{s} \\rangle - s\n",
    "\\end{align}\n",
    "where $\\langle  \\hat{s} \\rangle$ is the mean (or expected) value of the estimates. Note that  bias, as is true\n",
    "of many statistical quantities,  is a property not of an individual, here the estimate $\\hat{s}$, but rather the ensemble to\n",
    "which the individual is presumed to belong. An __estimator__, that is, a procedure that yields an estimate, is __consistent__ if the bias goes\n",
    "to zero as more and more data are included in the likelihood function. In practice, the expected value $\\langle  \\hat{s} \\rangle$ is approximated using a Monte Carlo method in which the estimating procedure (the estimator) is run a large number of times and the resulting estimates are \n",
    "averaged$^*$.\n",
    "\n",
    "Let $\\hat{s}$ be the MLE of $s$, which we shall take to be unbiased. *A priori*, we would expect $\\sqrt{N}$ to be a biased estimate of the standard\n",
    "deviation because $\\sqrt{N}$  is a nonlinear function of the estimate $\\hat{s} = N$.  First note that $\\hat{y} = g(\\hat{s}) = g(s + \\hat{s} - s)$, that is, \n",
    "$\\hat{y} = g(s + h)$, where $h = \\hat{s} - s$ is the (unknown) error. For small $h$, we can write\n",
    "\\begin{align}\n",
    "\\hat{y} \t&= g(\\hat{s}),\\nonumber\\\\\n",
    "\t\t\t&= g(s + h),\\nonumber\\\\\n",
    "\t\t\t&= g(s) + g^\\prime(s) h + \\frac{1}{2} g^{\\prime\\prime} (s) h^2 + O(h^3),\\nonumber\\\\\n",
    "\t\t\t&\\approx y + g^\\prime(s) h + \\frac{1}{2} g^{\\prime\\prime} (s) h^2.\n",
    "\\end{align}\n",
    "Now take the ensemble average of $\\hat{y}$,\n",
    "\\begin{align}\n",
    "\\langle \\hat{y} \\rangle\t\n",
    "\t\t&\\approx y + g^\\prime(s) \\langle h \\rangle + \n",
    "\\frac{1}{2} g^{\\prime\\prime}(s) \\langle h^2  \\rangle, \\nonumber\\\\\n",
    "\t\t&= y  + \n",
    "\\frac{1}{2} g^{\\prime\\prime} (s) \\textrm{V}.\n",
    "\\end{align}\n",
    "In the above, we have taken $\\langle h \\rangle$ to be zero, that is, we have assumed $\\hat{s}$ to be unbiased.\n",
    "\n",
    "We can draw the following conclusions from this result. First, the bias in $\\hat{y}$ is approximately $\\frac{1}{2} g^{\\prime\\prime} (s) \\textrm{V}$, where\n",
    "V is the variance. For the Poisson example, $\\textrm{V} = s$ and $g^{\\prime\\prime} (s) = -1/(4 s^{3/2})$,\n",
    "which yields  a negative bias in the estimate $\\hat{y} = \\sqrt{N}$ of approximately\n",
    "$-1/(8 \\sqrt{s})$. Second, we conform that the MLE of\n",
    "the standard deviation of a Poisson distribution is consistent; its bias is $\\propto 1/ \\sqrt{s}$ and therefore vanishes in the\n",
    "limit $s \\rightarrow \\infty$. Third, the bias cannot be determined exactly because it typically depends on unknown parameters, here the poisson mean $s$ for which only an estimate is available.\n",
    "\n",
    "Particle physicists tend to favor unbiased estimates and often correct the MLEs for bias. For the\n",
    "Poisson example, the obvious bias corrected estimate is\n",
    "\\begin{align}\n",
    "\\hat{y}_\\textrm{cor} &= \\sqrt{N} + \\frac{1}{8\\sqrt{N}}. \n",
    "\\end{align}\n",
    "You should ask whether correcting for bias makes sense. One reason why it may not make sense is that it may waste data. Ideally, given two estimates $\\hat{y}$ and $\\hat{y}_\\textrm{cor}$ one would hope that \n",
    "$\\langle (\\hat{y}_\\textrm{cor} - y)^2 \\rangle \\leq \\langle (\\hat{y} - y)^2 \\rangle$.\n",
    "Unfortunately, bias corrections can violate this condition, that is, yield, on average, less precise results. But, the size of the violation, and where it occurs, depends on the degree of nonlinearity of the function $y = g(s)$. As shown below, for the Poisson standard deviation, a bias correction may not be helpful for values of $s < 1.5$, but is helpful for values that are greater.\n",
    "\n",
    "\n",
    "*This is often described as running \"toy\" experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/06\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import ROOT\n",
    "%jsroot off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Poisson Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of experiments in ensemble:      25000\n",
      "range of Poisson mean s:     0.00 to     4.00\n",
      "number of steps in s:       20\n"
     ]
    }
   ],
   "source": [
    "rand  = ROOT.TRandom3()\n",
    "Nexp  = 25000\n",
    "smin  = 0.0\n",
    "smax  = 4.0\n",
    "nstep = 20\n",
    "step  = (smax-smin)/nstep\n",
    "print \"number of experiments in ensemble: %10d\" % Nexp\n",
    "print \"range of Poisson mean s: %8.2f to %8.2f\" % (smin, smax)\n",
    "print \"number of steps in s:    %5d\" % nstep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments\n",
    "For each value of the Poisson mean $s$, run a large number of experiments and accumulate statistics on them. The last column is the ratio of the mean squared error of the bias corrected estimate to that of the uncorrected estimate. This ratio is the amount by which the sample size would need to be increased in order for the corrected estimate to be as accurate as the uncorrected estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     s y=sqrt(s)  <y_hat>   bias_u   bias_c    RMS_u    RMS_c MSE_c/MSE_u\n",
      "  0.20      0.45     0.19   -0.257   -0.236     0.48     0.51        1.12\n",
      "  0.40      0.63     0.35   -0.278   -0.241     0.59     0.62        1.10\n",
      "  0.60      0.77     0.51   -0.266   -0.218     0.64     0.67        1.08\n",
      "  0.80      0.89     0.65   -0.243   -0.187     0.66     0.69        1.06\n",
      "  1.00      1.00     0.78   -0.219   -0.158     0.67     0.69        1.05\n",
      "  1.20      1.10     0.88   -0.214   -0.151     0.68     0.69        1.03\n",
      "  1.40      1.18     0.99   -0.190   -0.126     0.67     0.67        1.01\n",
      "  1.60      1.26     1.09   -0.178   -0.113     0.66     0.66        1.00\n",
      "  1.80      1.34     1.18   -0.160   -0.096     0.65     0.65        0.98\n",
      "  2.00      1.41     1.27   -0.148   -0.085     0.64     0.63        0.97\n",
      "  2.20      1.48     1.35   -0.129   -0.069     0.63     0.62        0.96\n",
      "  2.40      1.55     1.42   -0.125   -0.066     0.62     0.61        0.95\n",
      "  2.60      1.61     1.50   -0.112   -0.055     0.60     0.59        0.94\n",
      "  2.80      1.67     1.56   -0.109   -0.055     0.60     0.58        0.93\n",
      "  3.00      1.73     1.63   -0.102   -0.050     0.59     0.57        0.93\n",
      "  3.20      1.79     1.70   -0.088   -0.039     0.58     0.56        0.93\n",
      "  3.40      1.84     1.75   -0.092   -0.045     0.58     0.56        0.92\n",
      "  3.60      1.90     1.81   -0.088   -0.043     0.57     0.55        0.92\n",
      "  3.80      1.95     1.88   -0.074   -0.032     0.56     0.54        0.92\n",
      "  4.00      2.00     1.93   -0.074   -0.034     0.56     0.54        0.92\n",
      "100.00     10.00     9.99   -0.013   -0.012     0.50     0.50        1.00\n"
     ]
    }
   ],
   "source": [
    "print '%6s %9s %8s %8s %8s %8s %8s %11s' % \\\n",
    "  ('s', 'y=sqrt(s)', '<y_hat>', 'bias_u', 'bias_c',\n",
    "       'RMS_u', 'RMS_c', 'MSE_c/MSE_u')\n",
    "\n",
    "sqrt = ROOT.TMath.Sqrt\n",
    "\n",
    "# loop over Poisson mean s\n",
    "for ii in range(nstep+1):\n",
    "    \n",
    "    # true mean\n",
    "    if ii < nstep:\n",
    "        s = smin + (ii+1) * step\n",
    "    else:\n",
    "        s = 100.0\n",
    "    \n",
    "    # true standard deviation\n",
    "    y = sqrt(s)\n",
    "    \n",
    "    # for uncorrected estimate of sqrt(s)\n",
    "    y_u   = 0.0\n",
    "    MSE_u = 0.0\n",
    "\n",
    "    # for corrected estimate of sqrt(s)\n",
    "    y_c   = 0.0\n",
    "    MSE_c = 0.0\n",
    "    \n",
    "    # loop over experiments\n",
    "    for jj in range(Nexp):\n",
    "        \n",
    "        # run experiment\n",
    "        N = rand.Poisson(s)\n",
    "        \n",
    "        # MLE estimate of s\n",
    "        s_hat = N\n",
    "        \n",
    "        # MLE estimate of sqrt(s)\n",
    "        y_hat = sqrt(s_hat)\n",
    "        \n",
    "        # bias corrected estimate of sqrt(s)\n",
    "        y_hat_cor = y_hat\n",
    "        if s_hat > 0: y_hat_cor += 1.0/(8*s_hat)\n",
    "\n",
    "        # accumulate statistics\n",
    "        y_u   += y_hat\n",
    "        MSE_u +=(y_hat - y)**2\n",
    "\n",
    "        y_c   += y_hat_cor\n",
    "        MSE_c +=(y_hat_cor - y)**2\n",
    "        \n",
    "    # analyze ensemble of results\n",
    "    y_u    /= Nexp\n",
    "    MSE_u  /= Nexp; RMS_u = sqrt(MSE_u)\n",
    "\n",
    "    y_c    /= Nexp\n",
    "    MSE_c  /= Nexp; RMS_c = sqrt(MSE_c)\n",
    "\n",
    "    # compute bias\n",
    "    bias_u  = y_u - y\n",
    "    bias_c  = y_c - y\n",
    "\n",
    "    print '%6.2f %9.2f %8.2f %8.3f %8.3f %8.2f %8.2f %11.2f' % \\\n",
    "      (s, y, y_u, bias_u, bias_c, RMS_u, RMS_c, MSE_c/MSE_u)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One should not be surprised if a maximum likelihood estimate is biased when the data are few and counts are low.\n",
    "But, we should expect any sensible procedure for constructing estimates  to be consistent. After all, it \n",
    "would be a waste of resources to \n",
    "collect more data if results were unlikely to improve. Consistency is a more important requirement\n",
    "than whether or not a procedure for constructing estimates (an estimator) is biased, especially if a bias correction would make estimates appreciably less precise.\n",
    "\n",
    "Given two consistent procedures for estimating a parameter, how might one choose which to use? One way\n",
    "is to choose the procedure with the stronger consistency, that is, the procedure whose results converge faster\n",
    "to the true value of a parameter as more data are analyzed.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
