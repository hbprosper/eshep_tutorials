{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo (HMC)\n",
    " >__Created__:  2018 Harrison B. Prosper\n",
    "\n",
    "The standard approach in machine learning is to search for a single optimal function within the specified function space by minimizing the average loss. In the Bayesian approach, the task is construed as a problem of *inference* in which the goal is to assign a *probability density* to every function in the function space. The higher the probability density, the better the choice of function. But, the crucial point is that one is no longer obliged to choose a single function. In principle, one can use all of them provided that results are weighted by the probability densities associated with the functions. In practice, of course, we use a finite sample of functions. \n",
    "\n",
    "The basic idea in Bayesian inference is to assign such probability densities (or probabilities) using Bayes theorem\n",
    "\n",
    "$$ \\boxed{\\, p(w \\, | \\, D) = \\frac{p(D \\, | \\, w) \\, \\pi(w)}{p(D)}\\,}$$\n",
    "\n",
    "The function $p(D \\, | \\,w)$ is called the __likelihood function__ or likelihood for short. It is the probability\n",
    "model $p(x \\, | \\,w)$ of *potential* data $x$ evaluated at the actual data $D$. Since $D$ are constants, the likelihood is a function of the parameters $w$ only. The function $\\pi(w)$ is called the prior density or __prior__ for short. It models what is known, or assumed, about $w$ independently of the data $D$. Bayes theorem reminds us of the impossibility of arriving at a satisfactory solution without making *some* assumptions. The assumptions are encoded both within the likelihood and the prior. The function $p(w \\, | \\, D)$ is called the __posterior density__.\n",
    "\n",
    "Given a function $f(x, w)$, we can compute, for example, its moments\n",
    "\n",
    "$$f_{m}(x) = \\int f^m(x, w) \\, p(w \\, | \\, D) \\, dw, $$\n",
    "\n",
    "which  can be used to assign an uncertainty to $f(x, w)$, e.g. $\\sigma = \\sqrt{f_2 - f_1^2}$.\n",
    "\n",
    "### Markov Chain Sampling\n",
    "In practice, integrals such as $$\\int f^m(x, w) \\, p(w \\, | \\, D) \\, dw,$$ must be approximated. However, numerical integration is out of the question because the dimensionality of $w$ is too large. One approach is to approximate $p(w \\, | \\, D)$ by a multivariate Gaussian, but that approximation may not be good enough. The method of choice for truly hard problems is to *sample* points $w_i$ from some known probability density $g(w)$, for which sampling is fast and easy, and approximate the integral\n",
    "by the average\n",
    "\n",
    "$$f_{m}(x) \\approx \\frac{1}{N} \\sum_{n=1}^N f^m(x, w_n) \\left[\\frac{p(w_n \\, | \\, D)}{g(w_n)} \\right].$$\n",
    "\n",
    "However, in order to achieve maximum accuracy, we should set $g(w) = p(w \\, | \\, D)$. But, sampling from $p(w \\, | \\, D)$ directly is also a hard problem! The method of choice to solve it is Hamiltonian Monte Carlo (HMC), which is a variant of Markov chain Monte Carlo (MCMC) that is well-suited to very difficult sampling problems. \n",
    "\n",
    "Suppose we wish to sample $w$ from the probability density $p(w)$. The quantity $w$, which could be $n$-dimensional, can be thought of as characterizing the *state* of a system. The first MCMC algorithm, developed by Metropolis, generates a sequence $w_1, w_2, \\cdots$ such that in the limit of an infinite sequence, the distribution of $w_i$ converges to $p(w)$. The algorithm has two ingredients: a probabilistic rule for suggesting new states, called a __proposal function__, and the __Metropolis__ rule to decide whether or not to transition from the current state to the new. The proposal function must satisfy the principle of *detailed balance*, that is, the probability   $p(w_j \\, | \\, w_i) \\, p(w_i)$ to transition from state $i$ to state $j$ is equal to the probability $p( w_i \\, | \\, w_j) \\, p(w_j)$ to do the reverse. Note that if the joint probability $p(w_i, w_j)$ exists, detailed balance is an immediate consequence. \n",
    "\n",
    "The Metropolis rule is to accept a new state if its probability density $p(w_j)$ is greater than that of the current state $p(w_i)$, but, if the probability density of the new state is *lower*, accept the new state with probability $p(w_j) \\, / \\, p(w_i)$. In practice, the rule is to accept the new state if $u < p(w_j) \\, / \\, p(w_i)$, where $u \\sim \\textrm{uniform}(0, 1)$, that is, if $u$ is sampled from a uniform density on the unit interval. \n",
    "\n",
    "The basic idea of HMC is to map the problem of sampling from $p(w)$ to the problem of sampling the states $(q, p)$ of a particle moving in a potential $U(q)$ with kinetic energy $K(p)$, where $q$ and $p$ are the position and momentum of the particle, respectively. The states are sampled from the probability density\n",
    "\n",
    "$$p(w) \\propto g(q, p) \\propto \\exp(-H(q, p) \\, / \\, T),$$\n",
    "\n",
    "where $H(q, p) = U(q) + K(p)$ is the __Hamiltonian__ of the system (its energy function) and $T$ the absolute temperature (which we shall set = 1). The mapping is achieved by identifying $q \\equiv w$ as the position and $p \\equiv (dw/dt)/m$ as the momentum and\n",
    "\n",
    "$$U(q) = - \\ln p(w).$$\n",
    " \n",
    "Note that the larger the value of the Hamiltonian, the smaller the probability density $p(w)$ and, conversely, the smaller the value of $H(q, p)$ the greater the value of $p(w)$. Therefore, the Metropolis rule amounts to the following. If the particle moves to a state of lower energy, then accept that state. If it moves to a state of higher energy then accept that state with probability $p(w_j) \\, / \\, p(w_i)$. That is, we accept some wrong moves but with a probability that diminishes the higher the energy of the proposed new state relative to that of the current state.\n",
    "\n",
    "In statistical mechanics, the probability density $g(q, p)$ determines the thermodynamic \n",
    "properties of the system. Of course, we should really place quotes around position, momentum, etc. because the system is fictitious!\n",
    "\n",
    "Given a particle's Hamiltonian, its motion in classical physics can be computed using Hamilton's\n",
    "equations\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{dq}{dt} & = \\frac{\\partial H}{\\partial p},\\\\\n",
    "    \\frac{dp}{dt} & = -\\frac{\\partial H}{\\partial q},\n",
    "\\end{align*}\n",
    "\n",
    "which are just Newton's laws of motion in disguise.\n",
    "\n",
    "#### Hamiltonian Monte Carlo\n",
    "\n",
    "Hamiltonian Monte Carlo is an MCMC algorithm with a sophisticated proposal function based on Hamilton's equations. The proposal function consists of first randomly selecting the particle's momentum then moving the particle deterministically along a finite\n",
    "trajectory from state $i$ to state j in accordance with Hamiliton's equations. Then, the Metropolis rule is applied to decide whether to accept or reject the proposed state $(q, p)$. If the proposed state is rejected, we return to the start of the trajectory and try again with another randomly chosen momentum. If the state is accepted, we repeat the random selection of momentum followed by the deterministic move. The process is iterated for as many steps as needed to create a sample of points $\\{(q_i, p_i)\\}$ whose distribution faithfully matches $\\exp(-H(q, p))$. The values $p_i$ are discarded and the $q_i$, that is, $w_i$ are kept. \n",
    "\n",
    "The reason HMC works is because Newton's laws are time-reversal invariant, therefore, the proposal function satisfies detailed balance. The advantage of HMC is that the degree of random walking is reduced making for a more rapid and efficient exploration of the parameter space than methods based on a pure random walk as in the original Metropolis-Hastings algorithms. Every finite trajectory is randomly oriented with respect to its predecessor, but the trajectory itself is deterministic.\n",
    "\n",
    "The Hamilton equations are solved using a finite difference equation called the *leapfrog* algorithm. See below.\n",
    "\n",
    "\n",
    "### HMC Algorithm\n",
    "\n",
    "  1. Choose a random starting value $q = q_0$, a step size $\\epsilon$, a count $L$, and a standard deviation $\\sigma$.\n",
    "  3. Sample $p = p_0$ from a Gaussian of zero mean and some variance $\\sigma$ (p = np.random.normal(0, 1))\n",
    "  3. compute\n",
    "    $$p = p - \\frac{\\epsilon}{2} \\, \\frac{\\partial U(q)}{\\partial q}$$\n",
    "  3. __for__ $i = 0\\cdots L-1$ steps do:\n",
    "  \\begin{align*}\n",
    "  q & = q + \\epsilon \\, p\\\\\n",
    "    \\mathbf{if} \\, \\, i >&= L - 1: \\, \\, \\mathbf{break}\\\\\n",
    "  p & =  p - \\epsilon \\, \\frac{\\partial U(q)}{\\partial q}\\\\\n",
    "     \\mathbf{end \\, \\, for}\n",
    "  \\end{align*}\n",
    "  5. compute\n",
    "    $$p = p - \\frac{\\epsilon}{2} \\, \\frac{\\partial U(q)}{\\partial q}$$\n",
    "  6. compute\n",
    "  \\begin{align*}\n",
    "  U_0 & = U(q_0)\\quad & K_0 = \\textrm{sum}(p_0^2) \\, / \\, 2\\\\\n",
    "    U & = U(q) \\quad & K = \\textrm{sum}(p^2) \\, / \\, 2\\\\\n",
    "  \\end{align*}\n",
    "  7. Now decide whether to accept or reject the next point\n",
    "  \\begin{align*}\n",
    "      u & = \\textrm{np.random.uniform}(0,1)\\\\\n",
    "      \\mathbf{ if } \\, \\, u & < \\exp(U_0 + K_0 - U - K) \\, \\, \\mathbf{ accept } \\, \\, q \\, \\, \\mathbf{ else } \\, \\, q = q_0\n",
    "   \\end{align*}\n",
    "  \n",
    "Steps 2 to 7 are repeated a large number of times (thousands to hundreds of thousands) and, given that it takes time for the chain to converge to the desired distribution, $p(w)$, one typically discards some fraction of the initial points and uses the rest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a single HMC iteration\n",
    "The function $U(q)$ should follow numpy semantics, namely, if $q$ is a numpy array, $U(q)$ should return a numpy array with each element the the partial derivative of $U(q)$ with respect to an element of $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(U, q, h=1.e-6):\n",
    "    return 0.5*(U(q + h) - U(q - h))/h\n",
    "\n",
    "def next_point(U, q0, epsilon=0.01, L=200, sigma=1.0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p0 = np.random.normal(np.zeros(len(q0)), sigma)\n",
    "    except:\n",
    "        p0 = np.random.normal(0, sigma)\n",
    "    q  = q0\n",
    "    p  = p0\n",
    "    \n",
    "    p  = p - epsilon * grad(U, q) / 2\n",
    "    for i in range(L):\n",
    "        q = q + epsilon * p\n",
    "        if i >= L-1: break\n",
    "        p = p - epsilon * grad(U, q)\n",
    "    p  = p - epsilon * grad(U, q) / 2\n",
    "    \n",
    "    # check whether to accept or reject point\n",
    "    U0 = U(q0)\n",
    "    try:\n",
    "        K0 = sum(p0*p0) / 2\n",
    "    except:\n",
    "        K0 = p0*p0 / 2\n",
    "    H0 = U0 + K0\n",
    "    \n",
    "    U1 = U(q)\n",
    "    try:\n",
    "        K1  = sum(p*p) / 2\n",
    "    except:\n",
    "        K1  = p*p/2 \n",
    "    H1 = U1 + K1\n",
    "    \n",
    "    u  = np.random.uniform(0,1)\n",
    "    v  = np.exp(H0 - H1)\n",
    "    if u < v:\n",
    "        return (q, True)\n",
    "    else:\n",
    "        return (q0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
